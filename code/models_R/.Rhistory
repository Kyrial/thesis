cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
##enlever les layer en trop et mettre rate en 1
df_metrics <-subset(df_metrics, select = c(rate, conv1_1, conv1_2, conv2_1, conv2_2, conv3_1, conv3_2, conv3_3,conv4_1,conv4_2,conv4_3,conv5_1,conv5_2,conv5_3
,fc6_relu, fc7_relu))
#  df_metrics <- cbind((df_beauty),(df_metrics))
#   df_metrics <- as.data.frame(df_metrics)
#######Nico Ridge 2
k= nrow(df_metrics)
print(k)
matrix = as.matrix(df_metrics)
lambdas = c()
predictions = c()
for (i in 1:k){
print(i)
train = matrix[-i,]
test = matrix[i,]
x_train = train[,-1] ##df_beauty[-i,]
y_train = train[,1]
#print(i)
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
print(cv_train$lambda.min)
plot(cv_train, xvar='lambda')
model <- glmnet(x_train, y_train, alpha = 1, lambda = cv_train$lambda.min)
lambdas = c(lambdas, cv_train$lambda.min)
#elastic net
#model <- train(
#  rate ~., data = train, method = "glmnet",
#  trControl = trainControl("cv", number = 10),
#  tuneLength = 10
#)
#predictions:
x_test = test[-1]
prediction <- model %>% predict(x_test) %>% as.vector()
predictions = c(predictions, prediction)
}
matrix <- cbind(predictions ,matrix)
Rsquare = R2(matrix[,1], matrix[,2])
print(Rsquare)
#####################################################################################
# 1. DESCRIPTION:
#####################################################################################
#####################################################################################
# 2. LIBRAIRIES:
#####################################################################################
library("rjson")
library("readr")
library("purrr")
library("tidyr")
library("tibble")
library("plyr")
library("corrplot")
library("FactoMineR")
library("dplyr")
library(caret)
library(glmnet)
setwd("/home/renoult/Bureau/thesis/code/functions")
#####################################################################################
# 3. PARAMETRES: def analyse_metrics(model_name, bdd, weight, metric,k):
#####################################################################################
#####################################################################################
# 3.1 Parametres
#####################################################################################
#mettre Ã§a pas en dur a terme mais en paramÃ¨tres passÃ© au script python (ou pas?)
model_name <- 'VGG16'
bdd <- 'CFD_F'
weight <- 'imagenet'
metric <- 'gini_flatten'
#####################################################################################
# 3.2. Data management
#####################################################################################
layers = c('input_1',
'block1_conv1','block1_conv2','block1_pool',
'block2_conv1','block2_conv2','block2_pool',
'block3_conv1','block3_conv2','block3_conv3','block3_pool',
'block4_conv1','block4_conv2','block4_conv3','block4_pool',
'block5_conv1','block5_conv2','block5_conv3','block5_pool',
#'flatten',
'fc1', 'fc2'
)
#path d'enregistrement des rÃ©sultats et chargement des donnÃ©es
labels_path = paste('../../data/redesigned/',bdd,'/labels_',bdd,'.csv', sep='')
log_path =paste('../../results/',bdd,'/LLH_FeatureMap/LLH_',bdd,'_AllLLH.csv',sep = '')
#log_path =paste('../../results/',bdd,'/LLH_max/LLH_',bdd,'_AllLLH.csv',sep = '')
log_path =paste('../../results/',bdd,'/LLH_average/LLH_',bdd,'_AllLLH.csv',sep = '')
#log_path =paste('../../results/',bdd,'/LLH_average_model/LLH_',bdd,'_AllLLH.csv',sep = '')
#log_path =paste('../../results/',bdd,'/LLH/LLH_',bdd,'_AllLLH.csv',sep = '')
log_path_rate =paste('../../results/',bdd,'/log_', sep="")
matrix_metrics <- read_csv(file=log_path)
colnames(matrix_metrics)[2] <- 'input_1'
matrix_beauty <- do.call(cbind,read.csv(file=labels_path, header=FALSE))
colnames(matrix_beauty) <- c("img","rate")
df_beauty <-subset(matrix_beauty, select = c(rate))
# df_beauty$rate <-as.numeric(df_beauty$rate)
#on rÃ©cupÃ¨re les notes de beautÃ©
#  matrix_beauty <- do.call(cbind, fromJSON(file = paste(log_path_rate,'_',bdd,'_',weight,'_',metric,'_','_BRUTMETRICS','.csv',sep=""),simplify = FALSE))
#  df_beauty <- as.data.frame(matrix_beauty, optional = TRUE)
#  df_beauty = sapply(df_beauty, as.numeric)
#  df_beauty <- as.data.frame(df_beauty)
#  df = cbind(df_beauty$rate, matrix_metrics)
df = cbind(df_beauty, matrix_metrics)
#df <- plyr::rename(df, c("df_beauty$rate" = "rate"))
df <- df[,-2]
df_metrics = sapply(df, as.numeric)
df_metrics <- as.data.frame(df_metrics)
#################
# 5. MULTIPLES MODELS
#####################################################################################
if (weight %in% c('imagenet','vggplaces')) {
df_metrics = rename(df_metrics, c( "input_1" = "input_1" ,
'conv1_1' = 'block1_conv1',
'conv1_2' = 'block1_conv2',
'pool1' =  'block1_pool',
'conv2_1' =  'block2_conv1',
'conv2_2' = 'block2_conv2',
'pool2' =  'block2_pool',
'conv3_1' = 'block3_conv1',
'conv3_2' = 'block3_conv2',
'conv3_3' =  'block3_conv3',
'pool3' =  'block3_pool',
'conv4_1' = 'block4_conv1',
'conv4_2' = 'block4_conv2',
'conv4_3' = 'block4_conv3',
'pool4' = 'block4_pool',
'conv5_1' = 'block5_conv1',
'conv5_2' = 'block5_conv2',
'conv5_3' =  'block5_conv3',
'pool5' = 'block5_pool',
#      'flatten' = 'flatten',
'fc6_relu' = 'fc1',
'fc7_relu' = 'fc2'
))
}
print(paste('parameters are:',bdd,'-',weight,'-',metric, sep = ""))
#####################################################################################
#5.3. model with layers and interaction with complexity
#####################################################################################
#####################################################################################
# MODELE LINEAIRE
#####################################################################################
model = lm(rate ~ +conv1_1+conv1_2+conv2_1+conv2_2+conv3_1+conv3_2+conv3_3+conv4_1+conv4_2+conv4_3+conv5_1+conv5_2+conv5_3
+fc6_relu+fc7_relu
,data = df_metrics)
#ajouter les couches dense
print(summary(model))
##enlever les layer en trop et mettre rate en 1
df_metrics <-subset(df_metrics, select = c(rate, conv1_1, conv1_2, conv2_1, conv2_2, conv3_1, conv3_2, conv3_3,conv4_1,conv4_2,conv4_3,conv5_1,conv5_2,conv5_3
,fc6_relu, fc7_relu))
#  df_metrics <- cbind((df_beauty),(df_metrics))
#   df_metrics <- as.data.frame(df_metrics)
#######Nico Ridge 2
k= nrow(df_metrics)
print(k)
matrix = as.matrix(df_metrics)
lambdas = c()
predictions = c()
for (i in 1:k){
print(i)
train = matrix[-i,]
test = matrix[i,]
x_train = train[,-1] ##df_beauty[-i,]
y_train = train[,1]
#print(i)
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
#print(cv_train$lambda.min)
#plot(cv_train, xvar='lambda')
model <- glmnet(x_train, y_train, alpha = 1)# , lambda = cv_train$lambda.min)
lambdas = c(lambdas, cv_train$lambda.min)
#elastic net
#model <- train(
#  rate ~., data = train, method = "glmnet",
#  trControl = trainControl("cv", number = 10),
#  tuneLength = 10
#)
#predictions:
x_test = test[-1]
prediction <- model %>% predict(x_test) %>% as.vector()
predictions = c(predictions, prediction)
}
matrix <- cbind(predictions ,matrix)
Rsquare = R2(matrix[,1], matrix[,2])
print(Rsquare)
#####################################################################################
# 1. DESCRIPTION:
#####################################################################################
#####################################################################################
# 2. LIBRAIRIES:
#####################################################################################
library("rjson")
library("readr")
library("purrr")
library("tidyr")
library("tibble")
library("plyr")
library("corrplot")
library("FactoMineR")
library("dplyr")
library(caret)
library(glmnet)
setwd("/home/renoult/Bureau/thesis/code/functions")
#####################################################################################
# 3. PARAMETRES: def analyse_metrics(model_name, bdd, weight, metric,k):
#####################################################################################
#####################################################################################
# 3.1 Parametres
#####################################################################################
#mettre Ã§a pas en dur a terme mais en paramÃ¨tres passÃ© au script python (ou pas?)
model_name <- 'VGG16'
bdd <- 'MART'
weight <- 'imagenet'
metric <- 'gini_flatten'
#####################################################################################
# 3.2. Data management
#####################################################################################
layers = c('input_1',
'block1_conv1','block1_conv2','block1_pool',
'block2_conv1','block2_conv2','block2_pool',
'block3_conv1','block3_conv2','block3_conv3','block3_pool',
'block4_conv1','block4_conv2','block4_conv3','block4_pool',
'block5_conv1','block5_conv2','block5_conv3','block5_pool',
#'flatten',
'fc1', 'fc2'
)
#path d'enregistrement des rÃ©sultats et chargement des donnÃ©es
labels_path = paste('../../data/redesigned/',bdd,'/labels_',bdd,'.csv', sep='')
log_path =paste('../../results/',bdd,'/LLH_FeatureMap/LLH_',bdd,'_AllLLH.csv',sep = '')
#log_path =paste('../../results/',bdd,'/LLH_max/LLH_',bdd,'_AllLLH.csv',sep = '')
log_path =paste('../../results/',bdd,'/LLH_average/LLH_',bdd,'_AllLLH.csv',sep = '')
#log_path =paste('../../results/',bdd,'/LLH_average_model/LLH_',bdd,'_AllLLH.csv',sep = '')
#log_path =paste('../../results/',bdd,'/LLH/LLH_',bdd,'_AllLLH.csv',sep = '')
log_path_rate =paste('../../results/',bdd,'/log_', sep="")
matrix_metrics <- read_csv(file=log_path)
colnames(matrix_metrics)[2] <- 'input_1'
matrix_beauty <- do.call(cbind,read.csv(file=labels_path, header=FALSE))
colnames(matrix_beauty) <- c("img","rate")
df_beauty <-subset(matrix_beauty, select = c(rate))
# df_beauty$rate <-as.numeric(df_beauty$rate)
#on rÃ©cupÃ¨re les notes de beautÃ©
#  matrix_beauty <- do.call(cbind, fromJSON(file = paste(log_path_rate,'_',bdd,'_',weight,'_',metric,'_','_BRUTMETRICS','.csv',sep=""),simplify = FALSE))
#  df_beauty <- as.data.frame(matrix_beauty, optional = TRUE)
#  df_beauty = sapply(df_beauty, as.numeric)
#  df_beauty <- as.data.frame(df_beauty)
#  df = cbind(df_beauty$rate, matrix_metrics)
df = cbind(df_beauty, matrix_metrics)
#df <- plyr::rename(df, c("df_beauty$rate" = "rate"))
df <- df[,-2]
df_metrics = sapply(df, as.numeric)
df_metrics <- as.data.frame(df_metrics)
#################
# 5. MULTIPLES MODELS
#####################################################################################
if (weight %in% c('imagenet','vggplaces')) {
df_metrics = rename(df_metrics, c( "input_1" = "input_1" ,
'conv1_1' = 'block1_conv1',
'conv1_2' = 'block1_conv2',
'pool1' =  'block1_pool',
'conv2_1' =  'block2_conv1',
'conv2_2' = 'block2_conv2',
'pool2' =  'block2_pool',
'conv3_1' = 'block3_conv1',
'conv3_2' = 'block3_conv2',
'conv3_3' =  'block3_conv3',
'pool3' =  'block3_pool',
'conv4_1' = 'block4_conv1',
'conv4_2' = 'block4_conv2',
'conv4_3' = 'block4_conv3',
'pool4' = 'block4_pool',
'conv5_1' = 'block5_conv1',
'conv5_2' = 'block5_conv2',
'conv5_3' =  'block5_conv3',
'pool5' = 'block5_pool',
#      'flatten' = 'flatten',
'fc6_relu' = 'fc1',
'fc7_relu' = 'fc2'
))
}
print(paste('parameters are:',bdd,'-',weight,'-',metric, sep = ""))
#####################################################################################
#5.3. model with layers and interaction with complexity
#####################################################################################
#####################################################################################
# MODELE LINEAIRE
#####################################################################################
model = lm(rate ~ +conv1_1+conv1_2+conv2_1+conv2_2+conv3_1+conv3_2+conv3_3+conv4_1+conv4_2+conv4_3+conv5_1+conv5_2+conv5_3
+fc6_relu+fc7_relu
,data = df_metrics)
#ajouter les couches dense
print(summary(model))
##enlever les layer en trop et mettre rate en 1
df_metrics <-subset(df_metrics, select = c(rate, conv1_1, conv1_2, conv2_1, conv2_2, conv3_1, conv3_2, conv3_3,conv4_1,conv4_2,conv4_3,conv5_1,conv5_2,conv5_3
,fc6_relu, fc7_relu))
#  df_metrics <- cbind((df_beauty),(df_metrics))
#   df_metrics <- as.data.frame(df_metrics)
#######Nico Ridge 2
k= nrow(df_metrics)
print(k)
matrix = as.matrix(df_metrics)
lambdas = c()
predictions = c()
for (i in 1:k){
print(i)
train = matrix[-i,]
test = matrix[i,]
x_train = train[,-1] ##df_beauty[-i,]
y_train = train[,1]
#print(i)
cv_train <- cv.glmnet(x_train, y_train, alpha = 0) #alpha = 0 fait une ridge regression (1 si lasso)
#print(cv_train$lambda.min)
#plot(cv_train, xvar='lambda')
model <- glmnet(x_train, y_train, alpha = 1)# , lambda = cv_train$lambda.min)
lambdas = c(lambdas, cv_train$lambda.min)
#elastic net
#model <- train(
#  rate ~., data = train, method = "glmnet",
#  trControl = trainControl("cv", number = 10),
#  tuneLength = 10
#)
#predictions:
x_test = test[-1]
prediction <- model %>% predict(x_test) %>% as.vector()
predictions = c(predictions, prediction)
}
matrix <- cbind(predictions ,matrix)
Rsquare = R2(matrix[,1], matrix[,2])
print(Rsquare)
